---
title: "Exploratory Data Analysis - Jargon User Analytics"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    css: styles.css
---

```{r setup-eda, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(urltools) 
has_ggrepel <- require(ggrepel)
has_wordcloud <- require(wordcloud)
library(lubridate)
library(grid)

jargon_purple <- "#ae77f2"  # Main purple color
jargon_blue<- "#8890eb"  # Lighter purple for variations

# Load data
profiles <- read.csv("data/profiles_rows_cleaned.csv")
questions <- read.csv("data/questions_rows.csv")
words <- read.csv("data/words_rows.csv")
levels <- read.csv("data/levels_rows.csv")
websites <- read.csv("data/website_blacklist_rows.csv")

# Data summary table
library(tibble)
data_summary <- tibble(
  `Dataset` = c("Profiles", "Questions", "Words", "Levels", "Websites"),
  `Records` = c(nrow(profiles), nrow(questions), nrow(words), nrow(levels), nrow(websites)),
  `Description` = c(
    "User profiles and settings",
    "Generated practice questions",
    "Vocabulary entries and translations",
    "User progression through difficulty levels",
    "Websites where extension was disabled"
  )
)

# Profiles variables
profile_vars <- tibble(
  Variable = c("user_id", "level", "paused", "chrome_notifs", "language", 
               "last_question_time", "week_streak", "daily_streak", "daily_progress",
               "daily_goal", "density", "highlightStyle"),
  Type = c("Primary Key", "Integer", "Boolean", "Boolean", "String",
           "DateTime", "Integer", "Integer", "Integer",
           "Integer", "Integer", "String"),
  Description = c(
    "Unique identifier for each user",
    "Current proficiency level",
    "Extension status on Chrome",
    "Notification preferences",
    "Current selected language mode",
    "Timestamp of most recent question",
    "Consecutive weeks of activity",
    "Consecutive days of activity",
    "Questions completed today",
    "Target questions per day",
    "Frequency of questions",
    "Text selection preference"
  ),
  Notes = c(
    "Anonymized identifier",
    "Range: 1-10",
    "TRUE/FALSE (Default: TRUE)",
    "TRUE/FALSE",
    "e.g., 'GRE Vocabulary', 'TikTok Slang'",
    "UTC timezone",
    "",
    "",
    "Resets daily",
    "User-set goal",
    "Percentage of eligible sentences shown (0-100)",
    "'highlight' or 'underline'"
  )
)

# Questions variables
question_vars <- tibble(
  Variable = c("question_id", "user_id", "created_at", "sentence", "word",
               "language", "original_sentence", "options_array", "answered_at",
               "chosen_option", "user_rating"),
  Type = c("Primary Key", "Foreign Key", "DateTime", "Text", "String",
           "String", "Text", "Array of String", "DateTime",
           "String", "Integer"),
  Description = c(
    "Unique question identifier",
    "Associated user",
    "Question generation time",
    "Original selected text",
    "Target word for learning",
    "Transformation mode",
    "Source text",
    "Multiple choice options",
    "Completion timestamp",
    "User's answer",
    "Question quality rating"
  ),
  Notes = c(
    "",
    "References profiles",
    "UTC timezone",
    "English source content",
    "",
    "Selected language mode",
    "Pre-transformation content",
    "Even indices: options in target language; Odd indices: English translations",
    "NULL if unanswered",
    "NULL if unanswered",
    "Feature not yet implemented"
  )
)

# Words variables
words_vars <- tibble(
  Variable = c("created_at", "word", "language", "user_id", "translation", "status"),
  Type = c("DateTime", "String", "String", "Foreign Key", "Text", "String"),
  Description = c(
    "Word entry timestamp",
    "Target vocabulary",
    "Language mode",
    "Associated user",
    "English translation",
    "Learning status"
  ),
  Notes = c(
    "UTC timezone",
    "",
    "",
    "References profiles",
    "AI-generated translation",
    "Currently all set to 'learning'"
  )
)

# Levels variables
levels_vars <- tibble(
  Variable = c("user_id", "language", "level"),
  Type = c("Foreign Key", "String", "Integer"),
  Description = c(
    "Associated user",
    "Language mode",
    "Difficulty level"
  ),
  Notes = c(
    "References profiles",
    "",
    "Range: 1-10"
  )
)

# Websites variables
websites_vars <- tibble(
  Variable = c("user_id", "website"),
  Type = c("Foreign Key", "String"),
  Description = c(
    "Associated user",
    "Blocked URL"
  ),
  Notes = c(
    "References profiles",
    "Sites where Jargon is disabled"
  )
)

# For EDA plots
library(dplyr)
library(tidyr)
library(plotly)
library(DT)

# Prepare enhanced_profiles for EDA plots
question_counts <- questions %>% group_by(user_id) %>% summarise(generated_questions = n())
answer_counts <- words %>% group_by(user_id) %>% summarise(answered_questions = n())
blocked_websites <- websites %>% group_by(user_id) %>% summarise(blocked_sites = n())
distinct_levels <- levels %>% group_by(user_id) %>% summarise(levels_attempted = n_distinct(level))
enhanced_profiles <- profiles %>%
  left_join(question_counts, by = "user_id") %>%
  mutate(generated_questions = ifelse(is.na(generated_questions), 0, generated_questions)) %>%
  left_join(answer_counts, by = "user_id") %>%
  mutate(answered_questions = ifelse(is.na(answered_questions), 0, answered_questions)) %>%
  left_join(blocked_websites, by = "user_id") %>%
  mutate(blocked_sites = ifelse(is.na(blocked_sites), 0, blocked_sites)) %>%
  left_join(distinct_levels, by = "user_id") %>%
  mutate(levels_attempted = ifelse(is.na(levels_attempted), 0, levels_attempted)) %>%
  mutate(highlightStyle = if_else(highlightStyle == "highlight", 1, 0))
```

```{=html}
<div class="nav-header">
  <div class="nav-title">Jargon Analytics</div>
  <div class="nav-links">
    <a href="index.html" class="nav-inactive">Home</a>
    <a href="data_overview.html" class="nav-inactive">Data Overview</a>
    <a href="eda.html" class="nav-active">EDA</a>
    <a href="methods.html" class="nav-inactive">Methods</a>
    <a href="results.html" class="nav-inactive">Results</a>
    <a href="conclusions.html" class="nav-inactive">Conclusions</a>
  </div>
</div>
```

## Exploratory Data Analysis {.tabset}

Our exploratory data analysis examines patterns that inform both research questions about usage context and feature adoption. We organize our exploration into four main categories:

### Blocked Website Patterns

```{r website_pie, fig.width=8, fig.height=4}
library(plotly)
# Prepare data
website_categories <- websites %>%
  mutate(
    domain = domain(website),
    standardized_domain = case_when(
      grepl("salesforce.com$", domain) ~ "salesforce.com",
      TRUE ~ domain
    ),
    category = case_when(
      grepl("localhost|vercel.com|developers.google.com|dtang.dev|surge.sh", standardized_domain) ~ "Development Tools",
      grepl("youtube.com|instagram.com|medal.tv", standardized_domain) ~ "Social & Entertainment",
      grepl("salesforce.com|okta.com|chatgpt.com|imean.ai", standardized_domain) ~ "Business & AI Tools",
      grepl("wikipedia.org|readthedocs.io|edstem.org|ground-school", standardized_domain) ~ "Learning Resources",
      grepl("mail.google.com|linkedin.com", standardized_domain) ~ "Communication",
      grepl("coinbase.com", standardized_domain) ~ "Financial Services",
      TRUE ~ "Other"
    )
  )

# Create summary with group sizes and ranks
website_summary <- website_categories %>%
  group_by(category) %>%
  summarise(total_sites = n()) %>%
  arrange(desc(total_sites)) %>%
  mutate(
    sites_percent = round(total_sites / sum(total_sites) * 100, 1),
    group_rank = row_number()
  )

# Create interactive pie chart
plot_ly(website_summary, 
        labels = ~category, 
        values = ~total_sites,
        type = 'pie',
        textinfo = 'label+percent',
        hoverinfo = 'text',
        text = ~paste('Category:', category,
                    '<br>Sites:', total_sites,
                    '<br>Percentage:', sites_percent, '%'),
        marker = list(colors = c("#2E86C1", "#27AE60", "#F1C40F", 
                               "#E74C3C", "#9B59B6", "#F39C12", "#95A5A6"))) %>%
  layout(
    title = list(
      text = "Distribution of Blocked Websites by Category",
      y = 0.95
    ),
    showlegend = FALSE,
    margin = list(t = 80, b = 50, l = 50, r = 50),
    height = 400
  )
```

```{r website_bar, fig.width=8, fig.height=6}
# Create website frequency with proper ordering
website_freq <- website_categories %>%
  group_by(standardized_domain, category) %>%
  summarise(
    count = n(),
    .groups = 'drop'
  ) %>%
  # Join with group ranks
  left_join(select(website_summary, category, group_rank), by = "category") %>%
  # Sort by count first, then by group rank for ties
  arrange(desc(count), group_rank) %>%
  mutate(
    # Truncate domain names to last 15 characters
    display_domain = ifelse(
      nchar(standardized_domain) > 15,
      paste0("...", substr(standardized_domain, nchar(standardized_domain) - 14, nchar(standardized_domain))),
      standardized_domain
    )
  ) %>%
  # Ensure specific ordering for count 4 and 2
  arrange(case_when(
    count == 4 ~ 1,
    count == 2 ~ 2,
    TRUE ~ 3
  ), desc(count), group_rank) %>%
  # Create a factor with the correct order
  mutate(
    display_domain = factor(display_domain, levels = unique(display_domain))
  )

# Create interactive bar chart with proper ordering
plot_ly(website_freq, 
        x = ~display_domain, 
        y = ~count, 
        color = ~category,
        type = 'bar',
        text = ~paste('Full Domain:', standardized_domain,
                     '<br>Category:', category,
                     '<br>Count:', count),
        hoverinfo = 'text',
        colors = c("#2E86C1", "#27AE60", "#F1C40F", 
                  "#E74C3C", "#9B59B6", "#F39C12", "#95A5A6")) %>%
  layout(
    title = list(
      text = "Frequency of Individual Blocked Websites",
      y = 0.95
    ),
    xaxis = list(
      title = "",
      tickangle = 45,
      ticktext = ~display_domain,
      tickvals = ~display_domain,
      categoryorder = "array",
      categoryarray = ~display_domain
    ),
    yaxis = list(title = "Count"),
    showlegend = TRUE,
    legend = list(
      orientation = "h",
      xanchor = "center",
      x = 0.5,
      y = 1.15
    ),
    margin = list(t = 150, b = 100, l = 50, r = 50),
    height = 600
  )
```

<em style='color: black; font-size: 16px;'>Figure 5: Website Usage Analysis - Distribution of blocked websites by category (top) and frequency of individual websites (bottom)</em>

The analysis of blocked websites reveals distinct patterns in how users interact with the Jargon extension. Professional tools—particularly Salesforce and AI platforms—are the most frequently blocked, suggesting that users tend to avoid using Jargon during work-related activities. The presence of development environment blocks indicates that some users are technical professionals, though this group represents only a modest portion of the overall user base. Educational content also features prominently among blocked websites, with users often disabling the extension on documentation sites and learning platforms, possibly to maintain focus during concentrated study sessions.

However, it is important to note that there are only 27 blocked sites across 92 users. This limited usage suggests that the blocking feature is not widely utilized, and the current data may not be conclusive. Caution should be exercised when generalizing these findings, as they may not fully represent the broader user population.

### Language Mode Usage
```{r language_distribution, echo=FALSE}
library(plotly)
# Calculate language statistics
language_stats <- questions %>%
  group_by(language) %>%
  summarise(
    question_count = n(),
    unique_users = n_distinct(user_id)
  ) %>%
  arrange(desc(question_count)) %>%
  mutate(label = language)

# Color palette for languages (expand as needed)
language_colors <- c(
  "Spanish" = "#2E86C1",
  "GlizzyTalk" = "#27AE60",
  "GRE Vocabulary" = "#F1C40F",
  "Tamil" = "#E74C3C",
  "French" = "#9B59B6",
  "German" = "#F39C12",
  "Italian" = "#1ABC9C",
  "Korean" = "#D35400",
  "Croatian" = "#8E44AD",
  "Bulgarian" = "#2ECC71",
  "SAT Vocabulary" = "#E67E22",
  "Mandarin Chinese" = "#3498DB",
  "Portuguese" = "#16A085",
  "Russian" = "#C0392B",
  "Sinhala" = "#7F8C8D",
  "Swedish" = "#2C3E50",
  "Tiktok Slang" = "#E91E63",
  "Urdu" = "#795548"
)

# Interactive scatter plot
plot_ly(
  language_stats,
  x = ~unique_users,
  y = ~question_count,
  type = 'scatter',
  mode = 'markers+text',
  color = ~language,
  colors = language_colors,
  text = ~label,
  textposition = 'top center',
  marker = list(size = 12, line = list(width = 1, color = '#333')),
  hoverinfo = 'text',
  hovertext = ~paste(
    'Language:', language, '<br>',
    'Unique Users:', unique_users, '<br>',
    'Questions:', question_count
  )
) %>%
  layout(
    title = 'Language Mode Usage Patterns',
    xaxis = list(title = 'Number of Unique Users'),
    yaxis = list(title = 'Number of Questions Generated'),
    legend = list(title = list(text = 'Language Mode'))
  )
```

<em style='color: black; font-size: 16px;'>Figure 6: Scatter plot showing the relationship between user adoption and question generation across different language modes</em>

The scatter plot highlights key patterns in language mode usage:
- Spanish is the most active mode, with the highest number of questions (~800) and users (~30).
- GlizzyTalk and Tamil show moderate engagement (~300 questions each).
- Korean and GRE Vocabulary form a middle tier (~200 questions).
- Most other languages have low adoption, with fewer users and questions.
- Some modes (e.g., Tamil) have high question counts despite fewer users, indicating intensive use by dedicated learners.

Overall, while usage intensity and adoption vary widely across languages, traditional language learning modes drive most activity.

#### Words Frequency Analysis
```{r word_frequency_analysis, echo=FALSE}
library(plotly)
library(stringr)
set.seed(370)
custom_stopwords <- c(
  "the", "and", "for", "that", "with", "your", "this", "you", "are", "was",
  "were", "their", "can", "will", "have", "has", "had", "been", "would",
  "could", "should", "its", "it's", "they", "them", "these", "those", "from",
  "what", "when", "where", "who", "which", "why", "how", "all", "any", "both",
  "each", "few", "more", "most", "other", "some", "such", "than", "too",
  "very", "into", "also", "back", "else", "even", "here", "hers", "his",
  "just", "like", "more", "much", "must", "well", "were", "first", "being",
  "sometimes", "together", "billion", "one", "people", "think", "out", "there",
  "ongoing", "curious", "a", "an", "in", "on", "at", "to", "of", "is", "be", "by", "as", "or",
  "if", "so", "up", "my", "me", "do", "it", "no", "not", "yes", "yet",
  "said", "made", "went", "got", "did", "done", "make", "go", "get", "say",
  "the", "a", "an", "and", "but", "or", "for", "nor", "on", "at", "to", "of",
  "with", "in", "by", "about", "under", "above", "from", "after", "before"
)
custom_stopwords <- tolower(trimws(custom_stopwords))

single_words <- questions %>%
  filter(!is.na(original_sentence)) %>%
  mutate(clean_text = tolower(original_sentence),
         clean_text = gsub("[[:punct:]]", " ", clean_text),
         clean_text = gsub("[0-9]", " ", clean_text),
         clean_text = gsub("\\s+", " ", clean_text)) %>%
  pull(clean_text) %>%
  paste(collapse = " ") %>%
  strsplit(" ") %>%
  unlist() %>%
  subset(!. %in% custom_stopwords & nchar(.) > 2) %>%
  table() %>%
  as.data.frame() %>%
  setNames(c("word", "freq")) %>%
  arrange(desc(freq)) %>%
  head(30)

bigrams <- questions %>%
  filter(!is.na(original_sentence)) %>%
  mutate(clean_text = tolower(original_sentence),
         clean_text = gsub("[[:punct:]]", " ", clean_text),
         clean_text = gsub("[0-9]", " ", clean_text),
         clean_text = gsub("\\s+", " ", clean_text)) %>%
  pull(clean_text) %>%
  paste(collapse = " ") %>%
  {
    text <- .
    words <- unlist(strsplit(text, "\\s+"))
    words <- words[!words %in% custom_stopwords & nchar(words) > 2]
    bigrams <- paste(words[-length(words)], words[-1])
    bigrams[nchar(bigrams) > 5]
  } %>%
  as.data.frame() %>%
  setNames("word") %>%
  count(word, name = "freq") %>%
  filter(grepl("\\s", word), !grepl("^\\s|\\s$", word)) %>%
  arrange(desc(freq)) %>%
  head(30)

# Interactive bar for single words
plot_ly(single_words, x = ~reorder(word, freq), y = ~freq, type = 'bar', name = 'Words',
        hoverinfo = 'x+y', marker = list(color = jargon_purple)) %>%
  layout(title = 'Top 30 Words', xaxis = list(title = 'Word'), yaxis = list(title = 'Frequency'))

# Interactive bar for bigrams
plot_ly(bigrams, x = ~reorder(word, freq), y = ~freq, type = 'bar', name = 'Bigrams',
        hoverinfo = 'x+y', marker = list(color = jargon_purple)) %>%
  layout(title = 'Top 30 Bigrams', xaxis = list(title = 'Bigram'), yaxis = list(title = 'Frequency'))
```
<em style='color: black; font-size: 16px;'>Figure 7: Word frequency analysis showing common words (top) and word pairs (bottom) in learning content.</em>

Insights from Word and Phrase Frequency Analysis (based on the English original sentences selected for content generation):

- The most common words and word pairs (e.g., "currents," "ice," "churn," "concentric," "ice form," "churn water") suggest that users frequently select technical or scientific content for practice, possibly from educational or informational sources.
Descriptive and Process-Oriented Language:
- Many frequent terms describe physical processes or states (e.g., "breeze," "rolls," "floating ball," "gentle churn"), indicating an emphasis on dynamic or descriptive language in the learning material.
- The recurrence of similar words and phrases (e.g., "form," "water") implies that certain concepts or topics are repeatedly practiced, which may reflect user interests or the nature of the source material.

Overall, the word frequency analysis reveals that users are engaging most with scientific and descriptive content, focusing on process-oriented vocabulary and recurring technical terms.

### Temporal Patterns

#### {.tabset}
##### Daily Activity
```{r temporal_patterns_timeline, echo=FALSE}
library(plotly)
daily_data <- questions %>%
  mutate(date = as.Date(created_at)) %>%
  group_by(date) %>%
  summarise(total_questions = n(), unique_users = n_distinct(user_id))

p1 <- ggplot(daily_data) +
  geom_line(aes(x = date, y = total_questions), color = jargon_purple, size = 1) +
  labs(title = 'Daily Question Generation', x = 'Date', y = 'Number of Questions') +
  theme_minimal()

p2 <- ggplot(daily_data) +
  geom_line(aes(x = date, y = unique_users), color = jargon_blue, size = 1) +
  labs(title = 'Daily Active Users', x = 'Date', y = 'Number of Users') +
  theme_minimal()

ggplotly(p1)
ggplotly(p2)
```
<em style='color: black; font-size: 16px;'>Figure 8: Daily activity patterns showing question generation and active users with their respective averages (dashed lines) over the observation period, based on UTC timezone.</em>

##### Weekly Activity
```{r temporal_patterns_weekly, echo=FALSE}
library(lubridate)
library(plotly)
weekly_data <- questions %>%
  mutate(date = as.Date(created_at), day_of_week = wday(date, label = TRUE, abbr = FALSE)) %>%
  group_by(date, day_of_week) %>%
  summarise(total_questions = n(), unique_users = n_distinct(user_id), .groups = 'drop') %>%
  group_by(day_of_week) %>%
  summarise(avg_questions = mean(total_questions), avg_users = mean(unique_users))

p3 <- ggplot(weekly_data) +
  geom_col(aes(x = day_of_week, y = avg_questions), fill = jargon_purple, alpha = 0.7) +
  labs(title = 'Average Daily Questions by Day of Week', x = 'Day of Week', y = 'Avg Questions') +
  theme_minimal()

p4 <- ggplot(weekly_data) +
  geom_col(aes(x = day_of_week, y = avg_users), fill = jargon_blue, alpha = 0.7) +
  labs(title = 'Average Active Users by Day of Week', x = 'Day of Week', y = 'Avg Users') +
  theme_minimal()

ggplotly(p3)

ggplotly(p4)
```
<em style='color: black; font-size: 16px;'>Figure 9: Weekly activity patterns showing average questions generated and active users by day of week (UTC timezone), with error bars indicating standard error.</em>

####

The temporal analysis reveals several key patterns in user engagement, based on both daily and weekly activity (all timestamps in UTC):

- **Daily Trends:**: Question generation and active user counts fluctuate considerably day-to-day, with occasional spikes (up to 200 questions or 12 users), but most days remain below the average (12.5 questions, 2.2 users).This indicates a small but steady user base, with 1–5 active users on most days.

- **Weekly Trends:** Question generation is highest on Mondays, Tuesdays, and Wednesdays, then tapers off toward the weekend,suggesting users are more engaged during the workweek. There is substantial variability across days, as shown by the error bars.

Together, these patterns indicate that Jargon's usage is characterized by low but regular engagement, with activity peaking midweek and significant day-to-day variability. This suggests a core group of users who interact with the platform most during the workweek.

### User Engagement Distribution
```{r engagement_patterns, echo=FALSE}
library(tidyr)
library(plotly)
# Reshape data to long format for all metrics
engagement_long <- enhanced_profiles %>%
  pivot_longer(
    cols = c("generated_questions", "answered_questions", "blocked_sites", "levels_attempted"),
    names_to = "Metric",
    values_to = "Value"
  )

# Faceted violin plot with free y scales, all in the same color, no legend
p <- ggplot(engagement_long, aes(x = Metric, y = Value)) +
  geom_violin(fill = '#ae77f2', alpha = 0.7, trim = FALSE) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(
    title = "Distribution of User Engagement Metrics",
    x = "",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
    axis.text.x = element_blank(),
    axis.title = element_text(size = 12, face = "bold"),
    legend.position = "none"
  )

ggplotly(p)
```
<em style='color: black; font-size: 16px;'>Figure 10: Distribution of key engagement metrics across users, showing individual violin plots for each metric with median and interquartile range (IQR) statistics. Each plot uses a distinct color and includes summary statistics.</em>

The violin plots provide a clearer view of the distribution of user engagement metrics:

- **Generated Questions & Answered Questions:** Most users generate and answer only a small number of questions, as shown by the wide base near zero. A few users are highly active, producing a long tail of outliers with much higher counts.
- **Blocked Sites:** The vast majority of users do not block any sites (distribution concentrated at zero), with only a handful blocking more than one site.
- **Levels Attempted:** Most users attempt only one level, with very few exploring multiple levels. The distribution is sharply peaked at one, with a small tail for higher values.

Overall, the violin plots highlight that engagement is highly skewed: most users interact minimally, while a small subset are much more active or exploratory. This pattern is consistent across all four metrics.
