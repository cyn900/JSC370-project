---
title: "Midterm Report"
output:
  html_document: default
  pdf_document: default
date: "2025-03-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

```{r include=FALSE}
# Load necessary libraries
library(dplyr)
library(readr)
library(tidyr)
library(knitr)
library(ggplot2)
library(corrplot)
library(tm)
library(wordcloud)
library(lubridate)
```

## Introduction

Jargon, a startup founded by a friend about a year ago, introduced an innovative language learning Chrome extension (https://chromewebstore.google.com/detail/jargon/gghkanaadhldgmknmgggdgfaonhpppoj). I've recently joined the team as the new tech lead to spearhead technology initiatives. 

Jargon enhances the browsing experience by underlining text on websites and generating language practice questions from these selections, catering to users' active engagement. This tool is designed for students seeking to bolster their language studies through daily practice. 

Despite being in the market for eight months, Jargon only has 100+ downloads on the Chrome extension store. Jargon faces challenges with user adoption and potential market and product issues. To address these challenges, I have decided to begin by analyzing the company's database to gain insights that could drive improvements in our product and strategies. The primary question I aim to address is: What key factors influence user engagement and sustained usage of Jargon?

## Methods

The original developers of Jargon designed a database to track real-life usage and store relevant information on Supabase. I downloaded five CSV files (profiles, words, questions, levels, and websites) from Supabase on March 16, 2025 for analysis.

```{r include=FALSE}
profiles <- read.csv("profiles_rows_cleaned.csv")
words <- read.csv("words_rows.csv")
questions <- read.csv("questions_rows.csv")
levels <- read.csv("levels_rows.csv")
websites <- read.csv("website_blacklist_rows.csv")
```

The 1st CSV, profiles, contains 92 user profile information that includes the current settings for each user profile. Each user is represented by a unique ID, which serves as the primary key in this dataset. This dataset includes user_id, level (current proficiency level), paused (whether paused on Chrome or not), chrome_notifs (whether Chrome notifications are open), language (the current language chosen), last_question_time, week_streak, daily_streak, daily_progress, daily_goal, density (density of the questions displayed on the website), highlightStyle (underline or highlight), and bonus_points. The bonus points column was designed for a competition in which users could earn bonus points by following on Instagram, joining Discord, commenting, and participating in activities to spread the word on the Chrome extension store. Note that the username was originally stored in this dataset, but I manually deleted the name column to protect users' privacy.

```{r include=FALSE}
names(profiles)
head(profiles)
```

The 2nd CSV, websites, contains the websites users chose to block Jargon on. It includes user_id and website (url). The combination of these two serves as the primary key. There are 27 entries in total.
```{r include=FALSE}
names(websites)
head(websites)
```

The 3rd CSV, levels, contains all records of users' chosen languages to study. It includes user_id, language, and level. The combination of these three serves as the primary key. There are 117 entries in total.

```{r include=FALSE}
names(levels)
head(levels)
```

The 4th CSV, questions, contains all questions generated by Jargon. It includes created_at, user_id, sentence, word, question_id, language, original_sentence, answered_at (time when the user answers), chosen_option, and user_rating. The question_id serves as the primary key. There are 2442 entries in total.

```{r include=FALSE}
names(questions)
head(questions)
```

The 5th CSV, "words," contains the correct words for all the questions users have answered. It includes created_at, word, language, user_id, translation, and status. There are 1594 entries in total.

```{r include=FALSE}
names(words)
head(words)
```

My primary objective is to identify patterns in user behavior.I have enriched the profile datasets with additional information, including generated_questions (the number of questions generated) and answered_questions (answered by each user), enhanced_profiles (the number of websites they have blocked), and distinct_levels (the variety of levels they have attempted).

Furthermore, regarding highlightStyle, there are only two options: highlight or underline. Therefore, I have converted it to a boolean format where 1 represents highlight and 0 represents underline.

```{r include=FALSE}
# Count the number of questions generated and answered by each user
question_counts <- questions %>%
  group_by(user_id) %>%
  summarise(
    generated_questions = n(), 
  )

answer_counts <- words %>%
  group_by(user_id) %>%
  summarise(
    answered_questions = n(), 
  )

# Count the number of websites blocked by each user
blocked_websites <- websites %>%
  group_by(user_id) %>%
  summarise(
    blocked_sites = n()
  )

# Count the distinct levels attempted by each user
distinct_levels <- levels %>%
  group_by(user_id) %>%
  summarise(
    levels_attempted = n_distinct(level)
  )

# Merge the enhanced data back to the profiles dataset
# Replace NA values with zero in the merged data
enhanced_profiles <- profiles %>%
  left_join(question_counts, by = "user_id") %>%
  mutate(
    generated_questions = ifelse(is.na(generated_questions), 0, generated_questions),
  ) %>%
  left_join(answer_counts, by = "user_id") %>%
  mutate(
    answered_questions = ifelse(is.na(answered_questions), 0, answered_questions)
  ) %>%
  left_join(blocked_websites, by = "user_id") %>%
  mutate(
    blocked_sites = ifelse(is.na(blocked_sites), 0, blocked_sites)
  ) %>%
  left_join(distinct_levels, by = "user_id") %>%
  mutate(
    levels_attempted = ifelse(is.na(levels_attempted), 0, levels_attempted)
  )

enhanced_profiles <- enhanced_profiles %>%
  mutate(highlightStyle = if_else(highlightStyle == "highlight", 1, 0))
```

I am also interested in conducting some Natural Language Processing (NLP) analysis. Although Jargon supports multiple languages including Chinese, Spanish, and Tamil, I currently intend to narrow my focus to English content for a more straightforward analysis. This will specifically involve English content such as GRE Vocabulary, SAT Vocabulary, GlizzyTalk and TikTok Slang. The dataset containing English questions comprises 624 entries.

```{r echo=FALSE}
# Filter the dataset for entries where the language is english
english_questions <- questions %>%
  filter(language %in% c("GRE Vocabulary", "GlizzyTalk", "SAT Vocabulary", "Tiktok Slang"))
```

## Preliminary Results

### User Profile

Understanding user engagement is crucial for optimizing user experience and enhancing platform functionality. First, it is important to understand how user is using the product.

```{r echo=FALSE}
data_for_plot <- enhanced_profiles %>%
  select(daily_goal, density) %>%
  pivot_longer(cols = c(daily_goal, density), names_to = "Metric", values_to = "Value") %>%
  group_by(Metric, Value) %>%
  summarise(Count = n(), .groups = 'drop')

# Plotting the data using ggplot2
ggplot(data_for_plot, aes(x = Value, y = Count, fill = Metric)) +
  geom_col(show.legend = FALSE) + # Use geom_col() for bar charts
  facet_wrap(~ Metric, scales = "free_x") + # Facet by Metric with free scales for x-axis
  labs(title = "Distribution of Daily Goals and Density",
       x = "Value",
       y = "Frequency") +
  theme_minimal() + # Using a minimal theme for better visuals
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) # Rotate x-axis labels for readability

```

The bar chart displays the distribution of “Daily Goals” and “Density” across a range of values. For “Daily Goals,” most entries cluster at the lower end of the scale, indicating that a significant number of users set lower daily goals, with the frequency dramatically decreasing as the goal value increases. The majority of users have goals of 25, and very few set goals as high as 100.

Conversely, “Density” shows a starkly different distribution pattern, where a vast majority of observations cluster at a specific point, around 100, suggesting that most users interact with high density. Fewer users exhibit lower density values, indicating less interaction or engagement.

```{r echo=FALSE}
# Remove NA values from highlightStyle and chrome_notif
filtered_profiles <- enhanced_profiles %>%
  filter(!is.na(highlightStyle), !is.na(chrome_notifs))

# Transform highlightStyle into a categorical format
highlight_data <- filtered_profiles %>%
  mutate(category = "Highlight Style",
         value = if_else(highlightStyle == 1, "Highlight", "Underline")) %>%
  count(category, value)

# Transform chrome_notif into a categorical format
chrome_notif_data <- filtered_profiles %>%
  mutate(category = "Chrome Notifications",
         value = if_else(chrome_notifs == 1, "Enabled", "Disabled")) %>%
  count(category, value)

# Combine both datasets
pie_data <- bind_rows(highlight_data, chrome_notif_data)

# Create faceted pie charts
ggplot(pie_data, aes(x = "", y = n, fill = value)) +
  geom_bar(stat = "identity", width = 1) +  # Create the pie slices
  coord_polar(theta = "y") +  # Convert bar chart into a pie chart
  facet_wrap(~ category) +  # Facet by category
  labs(title = "Comparison of Highlight Style and Chrome Notifications",
       x = NULL, y = NULL, fill = "Category") +
  theme_void() +  # Remove background grid and axis labels
  theme(legend.position = "bottom")
```

Only a small portion of people disable the notification.

The pie chart depicts the distribution of highlight styles among users, revealing a strong preference for the Underline style compared to Highlight. Notably, Highlight is a relatively new feature introduced based on active user suggestions, while “Underline” remains the default option. This discrepancy in preference may be due to users not being fully aware of the new Highlight option.

I also created key metrics of user interactions within our platform, which include Generated Questions, Answered Questions, Blocked Sites, and Levels Attempted. These metrics are instrumental in identifying user behaviors, gauging the effectiveness of the content, and detecting areas that may require improvements or additional support.

```{r echo=FALSE}
# Create summary statistics for key metrics
summary_stats <- data.frame(
  Metric = c("Generated Questions", "Answered Questions", "Blocked Sites", "Levels Attempted"),
  Mean = c(
    round(mean(enhanced_profiles$generated_questions, na.rm = TRUE), 2),
    round(mean(enhanced_profiles$answered_questions, na.rm = TRUE), 2),
    round(mean(enhanced_profiles$blocked_sites, na.rm = TRUE), 2),
    round(mean(enhanced_profiles$levels_attempted, na.rm = TRUE), 2)
  ),
  Median = c(
    round(median(enhanced_profiles$generated_questions, na.rm = TRUE), 2),
    round(median(enhanced_profiles$answered_questions, na.rm = TRUE), 2),
    round(median(enhanced_profiles$blocked_sites, na.rm = TRUE), 2),
    round(median(enhanced_profiles$levels_attempted, na.rm = TRUE), 2)
  ),
  Maximum = c(
    max(enhanced_profiles$generated_questions, na.rm = TRUE),
    max(enhanced_profiles$answered_questions, na.rm = TRUE),
    max(enhanced_profiles$blocked_sites, na.rm = TRUE),
    max(enhanced_profiles$levels_attempted, na.rm = TRUE)
  ),
  Minimum = c(
    min(enhanced_profiles$generated_questions, na.rm = TRUE),
    min(enhanced_profiles$answered_questions, na.rm = TRUE),
    min(enhanced_profiles$blocked_sites, na.rm = TRUE),
    min(enhanced_profiles$levels_attempted, na.rm = TRUE)
  )
)

# Display the table using kable
kable(summary_stats, caption = "Descriptive Statistics for Key Variables")
```



These statistics illustrate a varied level of engagement across different metrics, with notable differences between mean and median values for several metrics suggesting skewed distributions. Such insights can guide targeted strategies to enhance user participation and address areas with lower engagement.

```{r echo=FALSE}

# Reshape data to long format
data_long <- enhanced_profiles %>%
  pivot_longer(
    cols = c("blocked_sites", "levels_attempted", "generated_questions", "answered_questions"),
    names_to = "Metric",
    values_to = "Value"
  )

# Updated plotting code
ggplot(data_long, aes(x = "", y = Value)) +  # Remove x-axis categories by setting x to ""
  geom_boxplot(fill = "lightblue", color = "darkblue", outlier.shape = 21, outlier.colour = "red", outlier.fill = "orange") +
  facet_wrap(~ Metric, scales = "free_y", ncol = 2) +  # Use only one column of facets
  labs(
    title = "Distribution of User Engagement Metrics",
    x = NULL,  # Remove x-axis label
    y = "Count of Events"
  ) +
  theme_minimal() +
  theme(
    strip.background = element_rect(fill = "lightblue"),
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(hjust = 0.5),
    axis.text.x = element_blank(),  # Hide x-axis text
    axis.ticks.x = element_blank()  # Hide x-axis ticks
  )
```

These box plots supported our finding from the statistics above, highlighting that while a majority of users engage at lower levels across various metrics, there are notable exceptions where some users show exceptionally high engagement. 


```{r echo=FALSE}
# Extract the relevant columns
data_subset <- enhanced_profiles[, c("daily_goal", "density", "highlightStyle","blocked_sites", "levels_attempted", "generated_questions", "answered_questions")]

# Calculate the correlation matrix
cor_matrix <- cor(data_subset, use = "complete.obs")  # use "complete.obs" to handle missing values if any

# Plotting the correlation matrix
corrplot(cor_matrix, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45,  # Text label color and rotation
         tl.cex = 0.6,  # Text label size
         cl.cex = 0.7,  # Color legend size
         addCoef.col = "black")  # Add correlation coefficients to the plot
```

The correlation matrix illustrates key relationships between user engagement metrics. Notably, “Generated Questions” and “Answered Questions” show a very strong correlation (0.92), indicating that users who generate more questions tend to answer more as well. “Levels Attempted” is significantly correlated with both “Generated Questions” (0.71) and “Answered Questions” (0.64), suggesting that more active users engage across multiple dimensions of the platform. The “HighlightStyle” variable shows moderate correlations with “Levels Attempted” (0.52) and “Generated Questions” (0.67), indicating that the choice of highlight style might be linked to higher user activity. Other metrics such as “Daily Goal,” “Density,” and “Blocked Sites” show weaker correlations with user activity metrics, suggesting less direct interaction with these elements.

### Question Generation

Answering questions while browsing on Chrome is at the core of our product. It is crucial for us to understand not only what content users are engaging with but also their experiences in generating and answering questions. This understanding helps us to continuously improve the relevance and usability of our content, ensuring that the platform effectively meets user needs and enhances their browsing experience.

```{r echo=FALSE}
language_data <- questions %>%
  count(language)  # 'count' automatically names the count column 'n'

# Create the bar plot
ggplot(language_data, aes(x = language, y = n, fill = language)) +
  geom_col() +  # geom_col is used to create bar plots; equivalent to geom_bar(stat = "identity")
  labs(title = "Number of Questions Generated per Language",
       x = "Language",
       y = "Number of Questions Generated") +
  theme_minimal() +  # Use a minimal theme for a cleaner look
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```

The bar chart visualizes the number of questions generated per language, revealing that certain languages experience significantly more user engagement than others. Notably, Spanish, GRE Vocabulary. and Tamil stand out with the highest numbers of questions generated, indicating robust activity in these language segments. The data suggests a potential focus area for community engagement efforts or resource allocation to stimulate activity in underrepresented languages.


```{r echo=FALSE}
num_questions <- questions %>%
  mutate(created_at = as.POSIXct(created_at, tz = "UTC", format = "%Y-%m-%d %H:%M:%OS")) %>%
  mutate(date = as.Date(created_at)) %>%
  group_by(date) %>%
  summarise(number_of_questions = n(), .groups = 'drop')  # Count questions per day

# Create a time series plot
ggplot(num_questions, aes(x = date, y = number_of_questions)) +
  geom_line(group = 1, color = "blue") +  # Line plot
  geom_point(color = "red") +  # Add points
  labs(title = "Number of Questions Created Over Time",
       x = "Date",
       y = "Number of Questions") +
  theme_minimal()  # Using a minimal theme for a cleaner look
```

The graph illustrates the daily frequency of question generation over several months. Notably, the data reveals sporadic spikes in activity, with the most significant peaks occurring around October. These surges suggest occasional days of high user engagement or specific events that prompted an increased number of questions. I need to confirm with other team members in the company to determine the causes of these spikes. Outside of these peaks, question creation activity remains relatively low.

```{r echo=FALSE}
# Convert the created_at column to datetime format
questions <- questions %>%
  mutate(created_at = as.POSIXct(created_at, format = "%Y-%m-%d %H:%M:%OS", tz = "UTC"))

# Identify the top 10 users with the most questions
top_users <- questions %>%
  count(user_id, sort = TRUE) %>%  # Count and sort in descending order
  top_n(10, n) %>%
  arrange(desc(n))  # Explicitly arrange from highest to lowest

# Filter data to include only these top users
top_users_data <- questions %>%
  filter(user_id %in% top_users$user_id)

# Anonymize user IDs by replacing them with a numeric index, preserving the order
anon_users <- top_users %>%
  mutate(anon_id = factor(paste0("User ", row_number()), levels = paste0("User ", 1:10)))

# Merge anonymized user IDs back into the dataset
top_users_data <- top_users_data %>%
  left_join(anon_users, by = "user_id")

# Create the timeline plot
ggplot(top_users_data, aes(x = created_at, y = anon_id, color = anon_id)) +
  geom_point(alpha = 0.6) +  # Plot each question creation event
  geom_line(aes(group = anon_id), alpha = 0.2) +  # Connect points with lines
  labs(title = "Top 10 Users' Question Creation Timeline (Sorted by Activity)",
       x = "Date",
       y = "User (Most to Least Active)",
       color = "User") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10))+
  theme(legend.position = "none")
```

The graph visualizes the question creation timelines of the top 10 most active users, sorted from most to least active. All users are sporadic in their activity, with noticeable gaps between their question submissions.

```{r echo=FALSE}
# Create a text corpus
corpus <- Corpus(VectorSource(english_questions$original_sentence))

# Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))  # Convert text to lowercase
corpus <- tm_map(corpus, removePunctuation)  # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)  # Remove all numbers
corpus <- tm_map(corpus, removeWords, stopwords("english"))  # Remove common stopwords

# Additional optional steps could include stemming, stripping whitespace, etc.
corpus <- tm_map(corpus, stripWhitespace)  # Strip unnecessary whitespace

# Generate the word cloud
wordcloud(words = corpus, scale = c(3, 0.5), max.words = 30, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
```

Using the word cloud, we’ve observed that many words appear with the same frequency. To derive more nuanced insights into common themes and understand the type of content people engage with on Jargon, it would be advantageous to implement a more sophisticated clustering model. This approach will help us identify underlying patterns in the data, enhancing our ability to tailor and optimize the content effectively.

## Summary

From my initial exploration, I found that despite having 92 users on record, the product is not being used as consistently as expected. While only a small portion of users have disabled notifications, which may contribute to lower engagement, the broader issue seems to be that users are not integrating Jargon into their daily browsing as originally envisioned. The initial assumption was that users would engage with language learning passively while browsing daily, but the data suggests otherwise.

### Next Step

To better understand why users are not consistently using Jargon, I will conduct a structured analysis using data-driven modeling techniques and natural language processing (NLP) to extract insights. The key focus areas will be content analysis and user segmentation, with specific methods to handle multilingual data.

1. Content Analysis: Identifying What Users Engage With

This analysis will investigate the types of content users interact with when using Jargon to determine if specific topics, websites, or language levels impact engagement.

Steps & Methods

	1.	Data Preprocessing & Cleaning
	
	•	Detect and classify languages using cld2 or textcat.
	•	Normalize and clean text data by applying language-specific preprocessing:
	•	English, Spanish, French: Use spaCy or nltk for tokenization and stopword removal.
	•	Chinese: Use jiebaR for tokenization.
	•	Remove punctuation, special characters, and unnecessary symbols.
	
	2.	Topic Modeling
	•	Apply Latent Dirichlet Allocation (LDA) to extract common themes from the articles and webpages users engage with.
	•	Use TF-IDF (Term Frequency-Inverse Document Frequency) to identify key terms in different languages.
	
	3.	Sentiment & Readability Analysis
	
	•	Perform sentiment analysis to assess whether positive or negative content affects engagement:
	•	English: Use syuzhet for sentiment scoring.
	•	Chinese: Use SnowNLP for sentiment classification.
	•	Calculate readability scores (e.g., Flesch-Kincaid for English) to determine if more complex content affects engagement levels.
	
	4.	Correlation with Engagement
	
	•	Compare extracted themes, sentiment scores, and readability scores against:
	•	Number of questions generated/answered

⸻

2. Segmentation of Active vs. Inactive Users

The goal is to identify behavioral differences between highly engaged users and those who rarely use Jargon.

Steps & Methods

	1.	User Segmentation Using Clustering
	
	•	Apply K-Means Clustering or Hierarchical Clustering to group users into different engagement levels.
	•	Identify behavioral patterns within each segment (e.g., whether highly active users prefer certain content or interact differently).
	
	3.	Predictive Modeling for Engagement
	
	•	Train a Random Forest Classifier or Logistic Regression Model to predict whether a user is likely to become an active or inactive user based on their behavior.

⸻

Final Goals

By conducting these analyses, I aim to:

	•	Identify patterns in content consumption to understand what types of material encourage engagement.
	•	Determine key engagement drivers by segmenting users based on their behavior.
	•	Improve Jargon’s user experience by refining recommendations and adapting learning methods to fit actual usage patterns.

This structured approach will provide actionable insights to enhance Jargon’s effectiveness and user retention.

